## Overview and Objectives
The proposed solution is written in Python and was executed using the free Kaggle GPU environments. The solution improves a microsoft-phi2 base model by 
implementing a RAG method, model fine-tuning and prompt engineering. The improved model was used to answer questions from the 3rd Generation Partnership Project (3GPP) documents.
The model achieved an accuracy of 0.622 on the private leaderboard

## ETL Processes
 a. The first process was to create a chroma-db for all the rel18 documents.
 
  i. The entire rel18 documents was used to build the chroma db
  
  ii. The embedding model used was the "BAAI/bge-small-en-v1.5" from hugging face
  
  iii. A chunk size of 128 and a chunk overlap of 20 was used
  
  iv. The entire process was executed on the Kaggle GPU environment (GPU T4x2) and it ran for 9596.9 seconds.
  
  v. Here is the link to the fully executed notebook (https://www.kaggle.com/code/tobby18/llmachromadb-indexing)
  
  vi. The output was a chroma.sqlite3 database with embeddings. The size is 7.03gb
  
 b. Create context for all the training questions.
 
    i. In this process, I created a context using the chroma-db (created in step a) for all the training questions. The context and the training questions are used to fine-tune the model
    
   ii.To create the context, the 3GPP release number in the questions were removed. This enhances the accuracy of the context generated
   
   iii. A similarity top_k was used to generate the context. k was set to 1 and similarity cut-off set to 0.5. The entire process was executed in 218 seconds (GPU T4x2)
   
   iv. Here is the link to the fully executed notebook. https://www.kaggle.com/code/tobby18/rag-inference-for-training
   
   v. The output is a pickle of the training questions and the context generated by the RAG 
   
 c. Train the model with the created context
 
    i. The output of step b is used to train the model. The fine tuning was done using the Peft QLoRA technique. 
    
   ii. The model was loaded with a 4-bit quantization
   
   iii. 4 trained models were created by changing the hyperparameters of the LoraConfig. 
   
   iv. Here is the link to the kaggle notebook https://www.kaggle.com/code/tobby18/rag-fine-tuning
   
   v. A training accuracy of 53.42% was recorded (other accuracy recorded are 52%, 50% and 53%). The training process took 8953.7 sec on the Kaggle GPU T4x2
   
   vi. The output is an adapter-config json file and a model.safetensor
 
 d. Inference using the trained model + RAG + prompt engineering

  i. The trained model (output of step c) was used for the inferencing (on the test questions)

  ii. The prompt used for the training is shown below:

   Provide a correct answer to a multiple choice question. Use only one option from A, B, C, D or E.
    {row['question']}
    A) {row['option 1']}
    B) {row['option 2']}
    C) {row['option 3']}
    D) {row['option 4']}
    {get_option_5(row)}
    {context}
    Answer:

   iii. The letter answer from the model (A, B, C, D or E) was then converted to  number (1,2,3,4 or 5)
   
   iv. Here is the link to the Kaggle notebook where the full code was executed https://www.kaggle.com/code/tobby18/rag-inferencing-submission 

   vi. The execution was done on a Kaggle GPU 100 environment for 18938.5s. The output is a csv of answers ready for submission


## Data Modelling

Four (4) different models were created by changing the QLoRA hyper-parameters, learning rate and batch size and number of training epochs.
Model 1: embedding model used was the "BAAI/bge-small-en-v1.5", lora_aplha=16, lora_dropout=0.01, r=2, batch_size=8, trained for 5 epochs 

Model 2: embedding model used was the "BAAI/bge-small-en-v1.5", lora_aplha=16, lora_dropout=0.01, r=2, batch_size=8, trained for 4 epochs 

Model 3: Changed the embedding model to the "BAAI/bge-large-en-v1.5", lora_aplha=16, lora_dropout=0.01, r=2, batch_size=4, trained for 4 epochs

Model 4:embedding model used was the "BAAI/bge-small-en-v1.5", lora_aplha=32, lora_dropout=0.05, r=16, batch_size=8, trained for 3 epochs

The four models were used to generate inferences. Voting technnique was conducted in the 4 inferences generated. This process is described in the next session.
The four models are available in the zipped folder.


## Inference

The answers from the 4 models were combined using a voting technique. The answer with the highest vote is chosen and if this is not available default to the answer from model 1.


## Performance and Metrics

The fine-tuned model achieved a score of 0.609289617 on the public leaderboard and a score of 0.622 on the private leaderboard
